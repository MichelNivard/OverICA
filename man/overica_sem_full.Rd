% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/functions.R
\name{overica_sem_full}
\alias{overica_sem_full}
\title{OverICA with Structural (I - B)^{-1}, Overcomplete Latent s, and Optional Error}
\usage{
overica_sem_full(
  data,
  k,
  moment_func,
  third = TRUE,
  error_cov = NULL,
  maskB = NULL,
  maskA = NULL,
  lambdaA = 0.01,
  lambdaB = 0,
  sigma = 0.01,
  hidden_size = 10,
  n_batch = 1024,
  use_adam = TRUE,
  adam_epochs = 100,
  adam_lr = 0.01,
  use_lbfgs = TRUE,
  lbfgs_epochs = 50,
  lbfgs_lr = 1,
  lbfgs_max_iter = 20,
  lr_decay = 0.999,
  clip_grad = TRUE,
  num_runs = 1
)
}
\arguments{
\item{data}{A numeric matrix of shape (n, p) for observed data.}

\item{k}{Number of latent sources s.}

\item{moment_func}{A function that takes a (n_batch x p) torch tensor and returns
a 1D torch tensor of statistics to match. (e.g. \code{torch_unique_4th_central_moments}).}

\item{third}{A logical the specifies whether third order moments are to be considered in the loss}

\item{error_cov}{An optional (p x p) covariance for Gaussian noise e. If NULL, no error added.}

\item{maskB}{Optional p x p binary mask for B. 1 => estimate the entry, 0 => fix to 0.}

\item{maskA}{Optional p x k binary mask for A. 1 => estimate, 0 => fix to 0.}

\item{lambdaA}{L1 penalty on A.}

\item{lambdaB}{L1 penalty on B.}

\item{sigma}{Covariance penalty weight for the sources s (to encourage whitening).}

\item{hidden_size}{Number of hidden units in the small MLP that maps z->s.}

\item{n_batch}{Batch size for the random z draws.}

\item{use_adam}{Whether to run Adam first.}

\item{adam_epochs}{Number of Adam epochs.}

\item{adam_lr}{Adam learning rate.}

\item{use_lbfgs}{Whether to run L-BFGS afterwards.}

\item{lbfgs_epochs}{Number of L-BFGS epochs (outer loop calls).}

\item{lbfgs_lr}{L-BFGS learning rate.}

\item{lbfgs_max_iter}{Max iteration per LBFGS step.}

\item{lr_decay}{Multiplicative LR decay per epoch (1 => no decay).}

\item{clip_grad}{Whether to clip the gradient norm to 2.}

\item{num_runs}{How many random restarts to do, picking the best final loss.}
}
\value{
A list with:
\item{best_result}{the run with lowest final loss}
\item{all_runs}{a list of all runs and their final parameters/loss}
}
\description{
This function implements a model:
data_hat = (I - B)^(-1) (s \%*\% A) + e
}
\details{
where:
\itemize{
\item B is a p x p matrix of direct causal/structural effects among the p obs. variables.
\item A is a p x k mixing matrix for the k latent sources s.
\item s is produced by a small neural net from z ~ Normal(0,I).
\item e ~ MVN(0, error_cov) is optional known noise (if error_cov is non-NULL).
}

We estimate B, A, and the net's parameters by matching the user-supplied "moment_func"
of data_hat to that of the observed data. Typically, you'd use e.g.
torch_unique_4th_central_moments() or torch_all_4th_central_moments() for 4th-order stats,
but you can supply any function returning a 1D torch vector for the mismatch.
}
